{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Kasonde Chewe\n",
    "    - Email: kc3745@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Ghanath V\n",
    "    - Email: gv347@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Kholoud Hamed M Al Nazzawi\n",
    "    - Email: ka974@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Group 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem C _(50 points)_\n",
    "\n",
    "This problem deals with finding \"pangrams\" in text. A pangram is a sentence containing all 26 letters of the alphabet. `x` and `y` in the cell below are example sentences, `x` is a pangram, `y` is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Jim quickly realized that the beautiful gowns are expensive.\"\n",
    "y = \"This sentence is most certainly not a pangram.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C1.__ _(5 points)_ Complete the function, `indices()`, that takes a string as input and outputs a list of the index numbers where each lowercase character from the English alphabet occurs first (only) in the string. Note: your function should `break` and `return` the list if `26` characters are found.\n",
    "\n",
    "[__Hint:__ you must keep track of which characters have been `found`, and a `set` could help with this. Also, you can compare letters like numbers. For example, `char >= \"a\"` is a valid conditional statement. You can use this to check whether characters in a string are letters of the alphabet.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1:Function(5/5) \n",
    "\n",
    "def indices(text):\n",
    "    first_indices = []; text = text.lower()\n",
    "    \n",
    "    #---your code starts here---\n",
    "\n",
    "    #---your code stops here---\n",
    "    \n",
    "    return first_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "[0, 1, 2, 4, 5, 7, 8, 9, 10, 12, 13, 14, 17, 19, 21, 22, 30, 36, 40, 41, 42, 43, 44, 51, 52, 57]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1:SanityCheck\n",
    "\n",
    "first_indices = indices(x)\n",
    "print(first_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ _(3 points)_ Now complete the function `verify()`, which must take a string as input and use the output of the `indices()` function to check if the string is a pangram, where the output of `verify()` should should be boolean `True` or `False` named `has_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2:Function(3/3)\n",
    "\n",
    "def verify(text):\n",
    "    has_all = False\n",
    "    first_indices = indices(text)\n",
    "    \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return has_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(True, False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "verify(x), verify(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C3:__ _(2 points)_ Now complete the below function named `tiny_verify()` that performs the check in a single line of code&mdash;_without using `indices()`_. [__Hint:__ Use a comprehension.] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3:Function(2/2)\n",
    "\n",
    "def tiny_verify(text):\n",
    "    \n",
    "    #---your single line of code starts here---\n",
    "    \n",
    "    #---your single line of code stops here---\n",
    "    \n",
    "    return has_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(True, False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "tiny_verify(x), tiny_verify(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C4.__ _(5 points)_ Now complete the `verify_missing()` by filling the `letters` set with any alphabetic characters that appear in the purported pangram. This version will return a list of missing letters instead of a boolean value, with the list of missing characters computed as a set difference against the `all_letters` set. [__Hint:__ Use the string containing all the letters in the alphabet, imported from the `string` module.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:Function(5/5)\n",
    "\n",
    "from string import ascii_lowercase as ascii_letters\n",
    "\n",
    "def verify_missing(text):\n",
    "    all_letters = set(ascii_letters)\n",
    "    letters = set()\n",
    "    \n",
    "    #---your code starts here---\n",
    "        \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return list(all_letters.difference(letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "([], ['z', 'v', 'k', 'd', 'u', 'j', 'w', 'b', 'f', 'x', 'q'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_missing(x), verify_missing(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C5.__ _(5 points)_ Now iterate through the loaded [list of pangrams](http://clagnut.com/blog/2380/) in `data/pangrams.txt` and `verify` which are actually pangrams. Store any incomplete sentences (that _don't_ `verify`) in the `imposters` list, which forms the function's output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C5:Function(5/5)\n",
    "\n",
    "potential_pangrams = open(\"data/pangrams.txt\", \"r\", encoding='UTF-8').readlines()\n",
    "potential_pangrams = [sentence.strip().lower() for sentence in potential_pangrams]\n",
    "\n",
    "def evaluate_pangrams(sentences):\n",
    "    imposters = []\n",
    "    \n",
    "    #---your code starts here---\n",
    "            \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return imposters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "Faulty pangram: \n",
    "Show mangled quartz flip vibe exactly.\n",
    "Missing letters: \n",
    "['k', 'j']\n",
    "\n",
    "Faulty pangram: \n",
    "Unamazingly, this six-word pangram is questionable!\n",
    "Missing letters: \n",
    "['c', 'v', 'k', 'j', 'f']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C5:SanityCheck\n",
    "\n",
    "imposters = evaluate_pangrams(potential_pangrams)\n",
    "for sentence in imposters:\n",
    "    print(\"Faulty pangram: \")\n",
    "    print(sentence)\n",
    "    print(\"Missing letters: \")\n",
    "    print(verify_missing(sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C6:__ _(3 points)_ Now complete the function below to use the output from the `verify()` function to fix the failed pangrams by any means necessary, and then collect the original and fixed sentences in a list of tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6:Function(3/3)\n",
    "\n",
    "import re\n",
    "import random as ra\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def tokenize(text, space = True, wordchars = \"a-zA-Z0-9-'\"):\n",
    "    tokens = []\n",
    "    for token in re.split(\"([\"+wordchars+\"]+)\", text):\n",
    "        if not space:\n",
    "            token = re.sub(\"[ ]+\", \"\", token)\n",
    "        if not token:\n",
    "            continue\n",
    "        if re.search(\"[\"+wordchars+\"]\", token):\n",
    "            tokens.append(token)\n",
    "        else: \n",
    "            tokens.extend(token)\n",
    "    return tokens\n",
    "\n",
    "def fix_pangrams(sentences, seed = 511):\n",
    "    word_counts = Counter([t for sentence in sentences for t in tokenize(sentence)])\n",
    "    word_index = defaultdict(list)\n",
    "    for w in word_counts: word_index[w[0]].append(w)\n",
    "    imposters_fixed = []\n",
    "    ra.seed(seed)\n",
    "    \n",
    "    #---your code starts here---\n",
    "            \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return imposters_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output could be:\n",
    "```\n",
    "Faulty pangram: \n",
    "show mangled quartz flip vibe exactly.\n",
    "Fixed pangram:\n",
    "show mangled quartz flip vibe exactly; kvetching jets.\n",
    "\n",
    "Faulty pangram: \n",
    "unamazingly, this six-word pangram is questionable!\n",
    "Fixed pangram:\n",
    "unamazingly, this six-word pangram is questionable; chimp veldt kazakh jonquils flip!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6:SanityCheck\n",
    "\n",
    "imposters_fixed = fix_pangrams(potential_pangrams)\n",
    "for sentence, fixed in imposters_fixed:\n",
    "    print(\"Faulty pangram: \")\n",
    "    print(sentence)\n",
    "    print(\"Fixed pangram:\")\n",
    "    print(fixed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C7.__ _(5 points)_ In the cell below, complete the metadata munging job using the information about the set of books in the `books_available` string. Assuming there may be many more books and that their information can be provided in the same format, create a data object that holds the book authors and titles associated to each book number, and writes the metadata as a JSON file in the `data/books/` directory using the following schema:\n",
    "\n",
    "`\n",
    "books = {\n",
    "    BookNumber: {\n",
    "        'author': AuthorName\n",
    "        'title': BookTitle,\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C7:Inline(5/5)\n",
    "\n",
    "import json\n",
    "\n",
    "books_available = \"\"\"\n",
    "84.txt; Frankenstein, or the Modern Prometheus; Mary Wollstonecraft (Godwin) Shelley\n",
    "98.txt; A Tale of Two Cities; Charles Dickens\n",
    "161.txt; Sense and Sensibility; Jane Austen\n",
    "730.txt; Oliver Twist or the Parish Boy's Progress; Charles Dickens\n",
    "768.txt; Wuthering Heights; Emily Brontë\n",
    "1322.txt; Leaves of Grass; Walt Whitman\n",
    "1342.txt; Pride and Prejudice; Jane Austen\n",
    "1400.txt; Great Expectations; Charles Dickens\n",
    "2701.txt; Moby Dick; or the Whale; Herman Melville\n",
    "4300.txt; Ulysses; James Joyce\n",
    "\"\"\"\n",
    "\n",
    "books = {}\n",
    "metadata_file = \"data/books/metadata.json\"\n",
    "\n",
    "#---your code starts here---\n",
    "\n",
    "#---your code stops here---\n",
    "\n",
    "json.dump({k: books[k] for k in sorted(books.keys())}, \n",
    "          open(metadata_file, \"w\"))\n",
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, you _could_ literally start typing:\n",
    "```\n",
    "books = {84 : {'title': 'Frankenstein, or the Modern Prometheus',\n",
    "               'author': 'Mary Wollstonecraft (Godwin) Shelley'}, ...}\n",
    "```\n",
    "but that would miss the point. Instead, you should use regular expressions to process the `books_available` object automatically by any convenient delimiters to produce the target object structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C8.__ _(4 points)_ Now complete the `sentokenize(text)` function using the `re` (regular expressions) module to split the book text into sentences using the `re.split(pattern, string)` function. The function must take a document and break it into 'sentences' (sentence-like strings), and then break these into lists of tokens within the larger `sentences` list.\n",
    "\n",
    "Note: you can efficiently use the below pattern to obtain the desired output for sentokenization:\n",
    "- `\"(\\s*(?<=[\"+delims+\"][^\"+sentchars+\"])\\s*)\"` \n",
    "    \n",
    "Additionally, ensure each `sentence` stored from the output of the sentokenization pattern is non-empty and processed by `tokenize` before being placed into the `sentences` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C8:Function(4/4)\n",
    "\n",
    "def sentokenize(text, space = True, delims = \".?!\", sentchars = \"a-zA-Z0-9-',;:\"):\n",
    "    sentences = []\n",
    "    \n",
    "    #---your code starts here---\n",
    "            \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "['Only', ' ', 'add', ' ', 'plain', ' ', 'text', ' ', 'in', ' ', 'the', ' ', 'designated', ' ', 'areas', ',', ' ', 'i', '.', 'e', '.', ',', ' ', 'replacing', ' ', 'the', ' ', 'relevant', ' ', \"'NA's\", '.', ' ', '\\n']\n",
    "['\\nCompletion of this header is mandatory, subject to a 2-point deduction to the assignment. \\n',\n",
    " \"Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. \\n\",\n",
    " 'You must fill out all group member Names and Drexel email addresses in the below markdown list, \\nunder header Module submission group. ',\n",
    " 'It is required to fill out descriptive notes pertaining to \\nany tutoring support received in the completion of this submission \\nunder the Additional submission comments section at the bottom of the header. \\n',\n",
    " 'If no tutoring support was received, leave NA in place. \\n',\n",
    " 'You may as well list other optional comments pertaining to the submission at bottom. \\n',\n",
    " \"Any distruption of this header's formatting will make your group liable to the 2-point deduction.\\n\"]   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C8:SanityCheck\n",
    "\n",
    "test_sentences = sentokenize(\"\"\"\n",
    "Completion of this header is mandatory, subject to a 2-point deduction to the assignment. \n",
    "Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. \n",
    "You must fill out all group member Names and Drexel email addresses in the below markdown list, \n",
    "under header Module submission group. It is required to fill out descriptive notes pertaining to \n",
    "any tutoring support received in the completion of this submission \n",
    "under the Additional submission comments section at the bottom of the header. \n",
    "If no tutoring support was received, leave NA in place. \n",
    "You may as well list other optional comments pertaining to the submission at bottom. \n",
    "Any distruption of this header's formatting will make your group liable to the 2-point deduction.\n",
    "\"\"\")\n",
    "\n",
    "print(test_sentences[1])\n",
    "[\"\".join(sentence) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C9.__ _(3 points)_ Now use the `sentokenize` function to complete the `get_pangrams(book_num)` function to determine which sentences in a given book are pangrams. The output of this function should be a tuple, continaing the list of found `pangrams` and the total number of sentencs (`num_sentences`) in the book. [Hint: don't forget to `.lower()` your sentences before attempting pangram verification!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C9:Function(3/3)\n",
    "\n",
    "def get_pangrams(book_num):\n",
    "    pangrams = []\n",
    "    num_sentences = 0\n",
    "    with open(\"data/books/\" + str(book_num) + \".txt\" , \"r\") as f:\n",
    "        for sentence in sentokenize(f.read()):\n",
    "\n",
    "            #---your code starts here---\n",
    "                \n",
    "            #---your code stops here---\n",
    "                \n",
    "    return pangrams, num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "8363 1 Every town-gate and village taxing-house had its band of\n",
    "citizen-patriots, with their national muskets in a most explosive state\n",
    "of readiness, who stopped all comers and goers, cross-questioned them,\n",
    "inspected their papers, looked for their names in lists of their own,\n",
    "turned them back, or sent them on, or stopped them and laid them in\n",
    "hold, as their capricious judgment or fancy deemed best for the dawning\n",
    "Republic One and Indivisible, of Liberty, Equality, Fraternity, or\n",
    "Death.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C9:SanityCheck\n",
    "\n",
    "book_results = get_pangrams(98)\n",
    "print(book_results[1], len(book_results[0]), \"\".join(book_results[0][0]) if book_results[0] else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C10.__ _(8 points)_ Now complete the below function to determine who is the pangrammiest author and what the pangrammiest book is, as determined by most pangrams per sentence. For this part, you must use the `defaultdict`s to store data in the `pangrams_by_author` and `pangrams_by_book` objects.  The first object (`pangrams_by_author`)  should be keyed by `author`, and the second object (`pangrams_by_book`) should be keyed by `(author, book_num)`-tuples. Each object's values should be total list of pangrams and sentence numbers for each grouping of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C10:Function(4/8)\n",
    "\n",
    "def collect_pangrams(books):\n",
    "    pangrams_by_author = defaultdict(lambda: [[], 0])\n",
    "    pangrams_by_book = defaultdict(lambda: [[], 0])\n",
    "\n",
    "    #---your code starts here---\n",
    "            \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return pangrams_by_author, pangrams_by_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(2, 26836)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C10:SanityCheck\n",
    "\n",
    "pangrams_by_author, pangrams_by_book = collect_pangrams(books)\n",
    "len(pangrams_by_author[\"Charles Dickens\"][0]), pangrams_by_author[\"Charles Dickens\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the function below to operate on the `pangrams_by_author` and `pangrams_by_book` objects and compute the portion of all sentences that were pangrams for each author-grouping and book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C10:Function(4/8)\n",
    "\n",
    "def compute_pangram_rates(pangrams_by_author, pangrams_by_book):\n",
    "    authors_pangrams_per_sentence, books_pangrams_per_sentence = [], []\n",
    "    \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return authors_pangrams_per_sentence, books_pangrams_per_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "\n",
    "```\n",
    "In order of decreasing pangrammyness, the authors are: \n",
    "[(0.00234375, 'Walt Whitman'),\n",
    " (0.0004151617054842861, 'James Joyce'),\n",
    " (0.0003994407829039345, 'Herman Melville'),\n",
    " (0.00034928396786587494, 'Emily Brontë'),\n",
    " (0.00023796303640834457, 'Jane Austen'),\n",
    " (7.452675510508272e-05, 'Charles Dickens'),\n",
    " (0.0, 'Mary Wollstonecraft (Godwin) Shelley')]\n",
    "\n",
    "In order of decreasing pangrammyness, the books are: \n",
    "[(0.00234375, 'Leaves of Grass'),\n",
    " (0.0004233700254022015, 'Pride and Prejudice'),\n",
    " (0.0004151617054842861, 'Ulysses'),\n",
    " (0.0003994407829039345, 'Moby Dick; or the Whale'),\n",
    " (0.00034928396786587494, 'Wuthering Heights'),\n",
    " (0.0001268874508311128, \"Oliver Twist or the Parish Boy's Progress\"),\n",
    " (0.00011957431543704412, 'A Tale of Two Cities'),\n",
    " (0.0, 'Great Expectations'),\n",
    " (0.0, 'Sense and Sensibility'),\n",
    " (0.0, 'Frankenstein, or the Modern Prometheus')]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C10:SanityCheck\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "authors_pangrams_per_sentence, books_pangrams_per_sentence = compute_pangram_rates(pangrams_by_author, \n",
    "                                                                                   pangrams_by_book)\n",
    "\n",
    "print(\"In order of decreasing pangrammyness, the authors are: \" )\n",
    "pprint(sorted(authors_pangrams_per_sentence,reverse = True))\n",
    "        \n",
    "print(\"\\nIn order of decreasing pangrammyness, the books are: \" )\n",
    "pprint([(x[0], books[x[1]]['title']) for x in sorted(books_pangrams_per_sentence,reverse = True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C11.__ _(7 points)_ Finally, complete the below function to compute the most efficient pangram and its author and book, as determined by fewest characters per pangram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C11:Function(7/7)\n",
    "\n",
    "def most_efficient_pangram(pangrams_by_book):\n",
    "    most_efficient_author =  None\n",
    "    most_efficient_book = None\n",
    "    most_efficient_title = None\n",
    "    least_characters = float(\"Inf\")\n",
    "    best_pangram = \"NA\"\n",
    "\n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "                \n",
    "    return (most_efficient_book, most_efficient_author, \n",
    "            most_efficient_title, least_characters, best_pangram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "The best pangramming author, Charles Dickens, wrote a 223-character pangram in the book: \n",
    "\"Oliver Twist or the Parish Boy's Progress\" (booknumber: 730)\n",
    "\n",
    "This pangram was: \n",
    "At least half a\n",
    "dozen more were severally drawn forth from the same box, and surveyed\n",
    "with equal pleasure; besides rings, brooches, bracelets, and other\n",
    "articles of jewellery, of such magnificent materials, and costly\n",
    "workmanship, that Oliver had no idea, even of their names.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C11:SanityCheck\n",
    "\n",
    "(most_efficient_book, most_efficient_author, \n",
    " most_efficient_title, least_characters, best_pangram) = most_efficient_pangram(pangrams_by_book)\n",
    "\n",
    "print(\"The best pangramming author, \" + most_efficient_author +\n",
    "      \", wrote a \" + str(least_characters) +\n",
    "      \"-character pangram in the book: \\n\\\"\" + most_efficient_title +\n",
    "      \"\\\" (booknumber: \" + str(most_efficient_book) +\")\\n\\n\" +\n",
    "      \"This pangram was: \\n\" + \"\".join(best_pangram))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
